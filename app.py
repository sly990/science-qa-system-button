import os
import streamlit as st
import torch
import pickle
import glob
import time
import uuid
from typing import List, Generator
from langchain_community.document_loaders import PDFPlumberLoader
from langchain.schema import Document
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_community.vectorstores import FAISS
from elasticsearch import Elasticsearch
import numpy as np
import traceback
from langchain_community.chat_models import ChatTongyi
import html

# é‡è¦ï¼šå¿…é¡»åœ¨ä»»ä½• Streamlit å‘½ä»¤ä¹‹å‰è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="ç§‘æ™®çŸ¥è¯†é—®ç­”åŠ©æ‰‹",
    layout="centered",
    page_icon="ğŸ”¬",
    initial_sidebar_state="collapsed"
)

# è§£å†³streamlitçš„å†²çªé—®é¢˜
torch.classes.__path__ = [os.path.join(torch.__path__[0], torch.classes.__file__)] 

# ======== æç¤ºè¯ç»Ÿä¸€ç®¡ç† ========
PROMPTS = {
    "is_scientific_query": [
        SystemMessage(content="è¯·åˆ¤æ–­ä»¥ä¸‹é—®é¢˜æ˜¯å¦å±äº'ç§‘æ™®ç›¸å…³å†…å®¹'ï¼ˆå¦‚ç§‘å­¦ã€æŠ€æœ¯ã€æ¼”è®²ç­‰ï¼‰ï¼Œè¯·åŠ¡å¿…åªå›ç­”'æ˜¯'æˆ–'å¦'ï¼š"),
        HumanMessage(content="{question}")
    ],
    "rag_answer": [
        SystemMessage(content="ä½ æ˜¯ç†æ€§ä¸”å‹å¥½çš„ç§‘å­¦é—®ç­”åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹ç§‘æ™®å†…å®¹ï¼Œåšä¸ªå…¨é¢çš„æ€»ç»“å’Œæ€è€ƒï¼Œå›ç­”ç”¨æˆ·é—®é¢˜ã€‚å¦‚æœæ‰¾ä¸åˆ°ç­”æ¡ˆï¼Œè¯·å›å¤'æŠ±æ­‰å“¦ï¼Œæˆ‘çš„çŸ¥è¯†åº“é‡Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„å†…å®¹ï¼'ã€‚"),
        HumanMessage(content="ä¸Šä¸‹æ–‡ï¼š\n{context}\n\né—®é¢˜ï¼š\n{question}")
    ],
    "simple_answer": [
        SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªæ´»æ³¼å¯çˆ±åˆç†æ€§çš„ç§‘æ™®é—®ç­”å°åŠ©æ‰‹ï¼Œè¯·ç®€æ´ã€å‹å¥½ã€ç¤¼è²Œåœ°å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š"),
        HumanMessage(content="{question}")
    ],
    "multi_turn": [
        SystemMessage(content="ä½ æ˜¯ç†æ€§ä¸”å‹å¥½çš„ç§‘å­¦é—®ç­”åŠ©æ‰‹ã€‚è¯·æ ¹æ®å¯¹è¯å†å²å’Œä»¥ä¸‹ç§‘æ™®å†…å®¹å›ç­”é—®é¢˜ï¼š\n{context}"),
        HumanMessage(content="å½“å‰å¯¹è¯å†å²ï¼š\n{history}\n\né—®é¢˜ï¼š\n{question}")
    ],
}

# é…ç½®è·¯å¾„å’Œç¯å¢ƒå˜é‡
DATA_DIR = os.path.join("data_pdf", "merged_pdfs")
VECTOR_STORE_PATH = os.path.join("vector_store", "vector_store.pkl")
ES_INDEX_NAME = "rag_docs"
os.environ['HF_ENDPOINT'] = "https://hf-mirror.com"

# è®¾ç½®æˆè‡ªå·±çš„å¯†é’¥
os.environ["TONGYI_API_KEY"] = "sk-506a5c243ac3445caea6be389b0025fb"  

# ç¨³å®šDOMæ›´æ–°çš„æ¶ˆæ¯ç®¡ç†å™¨
class ChatMessageManager:
    def __init__(self):
        self.messages = {}
        
    def add_message(self, role, content, placeholder=None):
        """æ·»åŠ æ–°æ¶ˆæ¯ï¼Œè¿”å›æ¶ˆæ¯IDå’Œå ä½ç¬¦"""
        message_id = str(uuid.uuid4())
        self.messages[message_id] = {
            "role": role,
            "content": content,
            "placeholder": placeholder or st.empty()
        }
        return message_id
        
    def update_message(self, message_id, new_content):
        """å®‰å…¨æ›´æ–°æ¶ˆæ¯å†…å®¹"""
        if message_id in self.messages:
            try:
                # ä½¿ç”¨HTMLè½¬ä¹‰ç¡®ä¿å†…å®¹å®‰å…¨
                safe_content = html.escape(new_content).replace("\n", "<br>")
                
                # æ„å»ºHTMLç»“æ„
                css_class = "user-message" if self.messages[message_id]["role"] == "user" else "assistant-message"
                html_content = f"<div class='{css_class}'>{safe_content}</div>"
                
                # æ›´æ–°å ä½ç¬¦å†…å®¹
                self.messages[message_id]["placeholder"].markdown(
                    html_content, 
                    unsafe_allow_html=True
                )
                return True
            except Exception:
                return False
        return False

# è·å–å‘é‡æ¨¡å‹
@st.cache_resource
def get_embeddings():
    try:
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        model_kwargs = {'device': device}
        embedder = HuggingFaceEmbeddings(
            model_name="liam168/bert-large-chinese",
            model_kwargs=model_kwargs,
            encode_kwargs={'normalize_embeddings': True}
        )
        _ = embedder.embed_query("æµ‹è¯•")
        return embedder
    except Exception:
        return HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")

# è¾…åŠ©å‡½æ•°ï¼šæ ¼å¼åŒ–æ¶ˆæ¯
def format_messages(template_messages, **kwargs):
    messages = []
    for msg in template_messages:
        content = msg.content.format(**kwargs)
        if isinstance(msg, SystemMessage):
            messages.append({"role": "system", "content": content})
        else:
            messages.append({"role": "user", "content": content})
    return messages

# è·å–é€šä¹‰åƒé—® LLM å®¢æˆ·ç«¯
def get_llm():
    api_key = os.getenv("TONGYI_API_KEY")
    if not api_key:
        st.error("é€šä¹‰APIå¯†é’¥æœªé…ç½®ï¼")
        return None
    try:
        return ChatTongyi(
            model_name="qwen-plus",
            dashscope_api_key=api_key,
            temperature=0.7,
            streaming=True
        )
    except Exception as e:
        st.error(f"é€šä¹‰æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
        return None

def load_documents() -> List[Document]:
    docs = []
    for file_path in glob.glob(f"{DATA_DIR}/*.pdf"):
        try:
            loader = PDFPlumberLoader(file_path)
            pages = loader.load()
            full_text = "\n".join([p.page_content for p in pages])
            file_name = os.path.basename(file_path)
            docs.append(Document(page_content=full_text, metadata={"source": file_name}))
        except Exception:
            continue
    return docs

# ç®€å•å‘é‡æ•°æ®åº“
class InMemorySimpleVectorStore:
    def __init__(self, embedding_function, texts, metadatas=None):
        self.embedding_function = embedding_function
        self.texts = texts
        self.metadatas = metadatas or [{} for _ in texts]
        self.doc_embeddings = []
        for text in texts:
            try:
                embedding = embedding_function.embed_query(text)
                self.doc_embeddings.append(embedding)
            except Exception:
                self.doc_embeddings.append(np.zeros(768, dtype=np.float32))

    def similarity_search(self, query, k=4):
        query_embedding = self.embedding_function.embed_query(query)
        query_vec = np.array(query_embedding, dtype=np.float32)
        similarities = []
        for doc_embedding in self.doc_embeddings:
            doc_vec = np.array(doc_embedding, dtype=np.float32)
            dot_product = np.dot(query_vec, doc_vec)
            query_norm = np.linalg.norm(query_vec)
            doc_norm = np.linalg.norm(doc_vec)
            similarity = dot_product / (query_norm * doc_norm) if query_norm > 0 and doc_norm > 0 else 0
            similarities.append(similarity)
        top_indices = np.argsort(similarities)[-k:][::-1]
        return [Document(page_content=self.texts[i], metadata=self.metadatas[i]) for i in top_indices]

# åŠ è½½å‘é‡æ•°æ®åº“
def load_or_create_vector_store():
    if os.path.exists(VECTOR_STORE_PATH):
        try:
            with open(VECTOR_STORE_PATH, "rb") as f:
                vector_store = pickle.load(f)
            return vector_store
        except Exception:
            return None
    return None

# æ„å»ºå‘é‡æ•°æ®åº“å’Œç´¢å¼•
@st.cache_resource
def build_vector_store_and_index():
    vector_store = load_or_create_vector_store()
    if vector_store:
        try:
            es = Elasticsearch("http://localhost:9200")
            es.info()
            return vector_store, es
        except Exception:
            return vector_store, None

    docs = load_documents()
    embedder = get_embeddings()
    if not embedder:
        return None, None

    docs = [doc for doc in docs if doc.page_content and doc.page_content.strip()]
    if not docs:
        return None, None

    try:
        try:
            vector_store = FAISS.from_documents(docs, embedder)
        except Exception:
            texts = [doc.page_content for doc in docs]
            metadatas = [doc.metadata for doc in docs]
            vector_store = InMemorySimpleVectorStore(embedder, texts, metadatas)

        os.makedirs(os.path.dirname(VECTOR_STORE_PATH), exist_ok=True)
        with open(VECTOR_STORE_PATH, "wb") as f:
            pickle.dump(vector_store, f)

        try:
            es = Elasticsearch("http://localhost:9200")
            es.info()
            if not es.indices.exists(index=ES_INDEX_NAME):
                actions = [{
                    "_index": ES_INDEX_NAME,
                    "_id": i,
                    "_source": {
                        "content": doc.page_content,
                        "source": doc.metadata.get("source", "unknown")
                    }
                } for i, doc in enumerate(docs)]
                for action in actions:
                    es.index(index=ES_INDEX_NAME, body=action['_source'])
            return vector_store, es
        except Exception:
            return vector_store, None

    except Exception:
        return None, None

# å…³é”®å­—åŒ¹é…        
def keyword_search(es, query, top_k=5):
    if not es:
        return []
    try:
        res = es.search(
            index=ES_INDEX_NAME,
            query={"match": {"content": query}},
            size=top_k
        )
        return [Document(page_content=hit["_source"]["content"], metadata={"source": hit["_source"]["source"]}) 
                for hit in res["hits"]["hits"]]
    except Exception:
        return []

# åˆ¤æ–­é—®é¢˜æ˜¯å¦ä¸ºç§‘æ™®é—®é¢˜
def is_scientific_query(question: str) -> bool:
    llm = get_llm()
    if not llm:
        return True
    try:
        messages = format_messages(PROMPTS["is_scientific_query"], question=question)
        response = llm.invoke(messages)
        if hasattr(response, 'content'):
            result = response.content
        else:
            result = str(response)
        result = result.strip().lower()
        return result.startswith("æ˜¯") or "æ˜¯" in result or result.startswith("yes")
    except Exception:
        return True

# æµå¼å“åº”ç”Ÿæˆå™¨
def generate_response_stream(user_input: str, is_sci_question: bool, rag_docs: List[Document] = None) -> Generator[str, None, None]:
    llm = get_llm()
    if not llm:
        yield "ç³»ç»Ÿé”™è¯¯ï¼šæ— æ³•åŠ è½½è¯­è¨€æ¨¡å‹ã€‚"
        return
    
    # æ„å»ºå¯¹è¯å†å²å­—ç¬¦ä¸²
    history_str = "\n".join(
        [f"{msg['role']}: {msg['content']}" 
         for msg in st.session_state.chat_history]
    )
    
    if is_sci_question and rag_docs:
        # æ„å»ºRAGä¸Šä¸‹æ–‡ï¼ˆé™åˆ¶é•¿åº¦ï¼‰
        context = "\n".join(
            [f"æ–‡æ¡£({i+1}): {doc.page_content[:500]}" 
             for i, doc in enumerate(rag_docs[:3])]  # åªå–å‰3ä¸ªæ–‡æ¡£ï¼Œé™åˆ¶é•¿åº¦
        )
        
        # ä½¿ç”¨å¤šè½®å¯¹è¯æ¨¡æ¿
        messages = format_messages(
            PROMPTS["multi_turn"],
            context=context,
            history=history_str,
            question=user_input
        )
    else:
        # æ™®é€šå¯¹è¯ä½¿ç”¨ç®€å•æ¨¡æ¿
        messages = format_messages(
            PROMPTS["simple_answer"],
            question=user_input
        )
        # æ·»åŠ å†å²ä¸Šä¸‹æ–‡
        messages.insert(0, {"role": "system", "content": f"å¯¹è¯å†å²ï¼š\n{history_str}"})
    
    try:
        # ä½¿ç”¨æµå¼è°ƒç”¨æ¨¡å‹
        response_stream = llm.stream(messages)
        
        for chunk in response_stream:
            if hasattr(chunk, 'content'):
                content = chunk.content
            else:
                content = str(chunk)
            
            yield content
            
    except Exception as e:
        yield f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™ï¼š{str(e)}"

# ... å‰é¢çš„ä»£ç ä¿æŒä¸å˜ ...

# ... å‰é¢çš„ä»£ç ä¿æŒä¸å˜ ...

def main():
    # åˆå§‹åŒ–æ‰€æœ‰å¿…è¦çš„ä¼šè¯çŠ¶æ€å˜é‡
    required_states = [
        "high_contrast_mode",
        "chat_history",
        "vector_store",
        "es",
        "message_manager"
    ]
    
    for state in required_states:
        if state not in st.session_state:
            # åˆå§‹åŒ–é«˜å¯¹æ¯”åº¦æ¨¡å¼
            if state == "high_contrast_mode":
                st.session_state.high_contrast_mode = True
            
            # åˆå§‹åŒ–èŠå¤©å†å²
            elif state == "chat_history":
                st.session_state.chat_history = [
                    {"role": "system", "content": "ä½ æ˜¯ç†æ€§ä¸”å‹å¥½çš„ç§‘å­¦é—®ç­”åŠ©æ‰‹"}
                ]
            
            # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
            elif state == "vector_store":
                st.session_state.vector_store = None
            
            # åˆå§‹åŒ–Elasticsearch
            elif state == "es":
                st.session_state.es = None
            
            # åˆå§‹åŒ–æ¶ˆæ¯ç®¡ç†å™¨
            elif state == "message_manager":
                st.session_state.message_manager = ChatMessageManager()
    
    # æ ¹æ®é«˜å¯¹æ¯”åº¦æ¨¡å¼çŠ¶æ€é€‰æ‹©CSSæ ·å¼
    if st.session_state.high_contrast_mode:
        css_style = """
        <style>
            body {
                background-color: #000000;
                color: #FFFFFF;
                font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            }
            .stChatMessage {
                background-color: #000000!important;
            }
            .user-message {
                padding: 12px;
                border-radius: 8px;
                margin: 8px 0;
                border-left: 4px solid #00FFFF; /* é’è‰²è¾¹æ¡† */
                background-color: #111111;
                color: #FFFFFF;
                font-size: 16px;
            }
            .assistant-message {
                padding: 12px;
                border-radius: 8px;
                margin: 8px 0;
                border-left: 4px solid #FFA500; /* æ©™è‰²è¾¹æ¡† */
                background-color: #111111;
                color: #FFFFFF;
                font-size: 16px;
            }
            .stTextInput > div > div > input {
                color: #FFFFFF;
                background-color: #222222;
                border-radius: 8px;
                padding: 10px;
                font-size: 16px;
            }
            .stButton > button {
                background-color: #FFA500!important;
                color: #000000!important;
                border-radius: 8px;
                padding: 8px 16px;
                font-weight: bold;
            }
            .stTitle {
                color: #FFFFFF;
                text-align: center;
                font-size: 24px;
                margin-bottom: 20px;
            }
            .toggle-btn {
                background-color: #FFA500!important;
                color: #000000!important;
                border-radius: 8px;
                padding: 8px 16px;
                font-weight: bold;
            }
            /* å»é™¤Streamlité»˜è®¤å…ƒç´  */
            .stApp .reportview-container .main .block-container {
                padding-top: 2rem;
                padding-right: 1rem;
                padding-left: 1rem;
                padding-bottom: 2rem;
            }
            #MainMenu {visibility: hidden;}
            footer {visibility: hidden;}
            header {visibility: hidden;}
        </style>
        """
    else:
        css_style = """
        <style>
            body {
                background-color: #FFFFFF;
                color: #333333;
                font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            }
            .stChatMessage {
                background-color: #FFFFFF!important;
            }
            .user-message {
                padding: 12px;
                border-radius: 8px;
                margin: 8px 0;
                border-left: 4px solid #007BFF;
                background-color: #E6F0FF;
                color: #333333;
                font-size: 16px;
            }
            .assistant-message {
                padding: 12px;
                border-radius: 8px;
                margin: 8px 0;
                border-left: 4px solid #28A745;
                background-color: #E6F7EB;
                color: #333333;
                font-size: 16px;
            }
            .stTextInput > div > div > input {
                color: #333333;
                background-color: #FFFFFF;
                border: 1px solid #CCCCCC;
                border-radius: 8px;
                padding: 10px;
                font-size: 16px;
            }
            .stButton > button {
                background-color: #007BFF!important;
                color: #FFFFFF!important;
                border-radius: 8px;
                padding: 8px 16px;
                font-weight: bold;
            }
            .stTitle {
                color: #333333;
                text-align: center;
                font-size: 24px;
                margin-bottom: 20px;
            }
            .toggle-btn {
                background-color: #007BFF!important;
                color: #FFFFFF!important;
                border-radius: 8px;
                padding: 8px 16px;
                font-weight: bold;
            }
            /* å»é™¤Streamlité»˜è®¤å…ƒç´  */
            .stApp .reportview-container .main .block-container {
                padding-top: 2rem;
                padding-right: 1rem;
                padding-left: 1rem;
                padding-bottom: 2rem;
            }
            #MainMenu {visibility: hidden;}
            footer {visibility: hidden;}
            header {visibility: hidden;}
        </style>
        """
    
    st.markdown(css_style, unsafe_allow_html=True)
    
    # åˆ›å»ºå®¹å™¨ç”¨äºæŒ‰é’®å¸ƒå±€
    button_container = st.container()
    with button_container:
        col1, col2 = st.columns([0.8, 0.2])
        with col1:
            st.title("ğŸŒŸ ç§‘æ™®çŸ¥è¯†é—®ç­”åŠ©æ‰‹")
        with col2:
            if st.session_state.high_contrast_mode:
                button = st.button("é€€å‡ºé«˜å¯¹æ¯”åº¦æ¨¡å¼", key="contrast_toggle", 
                                help="ç‚¹å‡»è¿”å›æ ‡å‡†æ˜¾ç¤ºæ¨¡å¼")
            else:
                button = st.button("è¿›å…¥é«˜å¯¹æ¯”åº¦æ¨¡å¼", key="contrast_toggle", 
                                help="ç‚¹å‡»åˆ‡æ¢ä¸ºé«˜å¯¹æ¯”åº¦æ˜¾ç¤º")
    
    # æ˜¾ç¤ºç•Œé¢è¯´æ˜ï¼ˆæ ¹æ®å½“å‰æ¨¡å¼è°ƒæ•´æ ·å¼ï¼‰
    if st.session_state.high_contrast_mode:
        st.markdown("""
        <div style="background-color:#111111; padding:15px; border-radius:8px; border-left:4px solid #FFA500; margin-bottom:20px;">
            <p>ğŸ’¡ğŸ’¡ <b>æ¬¢è¿ä½¿ç”¨ç§‘æ™®çŸ¥è¯†é—®ç­”åŠ©æ‰‹ï¼</b></p>
            <p>ğŸ”ğŸ” è¯·è¾“å…¥æ‚¨çš„ç§‘å­¦é—®é¢˜ï¼Œæˆ‘ä¼šå°½åŠ›æä¾›ä¸“ä¸šå›ç­”</p>
        </div>
        """, unsafe_allow_html=True)
    else:
        st.markdown("""
        <div style="background-color:#E6F7EB; padding:15px; border-radius:8px; border-left:4px solid #28A745; margin-bottom:20px;">
            <p>ğŸ’¡ğŸ’¡ <b>æ¬¢è¿ä½¿ç”¨ç§‘æ™®çŸ¥è¯†é—®ç­”åŠ©æ‰‹ï¼</b></p>
            <p>ğŸ”ğŸ” è¯·è¾“å…¥æ‚¨çš„ç§‘å­¦é—®é¢˜ï¼Œæˆ‘ä¼šå°½åŠ›æä¾›ä¸“ä¸šå›ç­”</p>
        </div>
        """, unsafe_allow_html=True)
    
    # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
    if st.session_state.vector_store is None:
        with st.spinner("æ­£åœ¨åŠ è½½çŸ¥è¯†åº“ï¼Œè¯·ç¨å€™..."):
            st.session_state.vector_store, st.session_state.es = build_vector_store_and_index()
            if st.session_state.vector_store is None:
                st.error("âš ï¸ å‘é‡æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥")
            if st.session_state.es is None:
                st.warning("âš ï¸ Elasticsearchè¿æ¥å¤±è´¥")
    
    # æ˜¾ç¤ºèŠå¤©å†å²
    for msg in st.session_state.chat_history:
        if msg["role"] != "system":
            # ä½¿ç”¨Streamlitå†…ç½®ç»„ä»¶æ˜¾ç¤ºæ¶ˆæ¯
            with st.chat_message(msg["role"]):
                st.markdown(msg['content'])
    
    # ç”¨æˆ·è¾“å…¥
    user_input = st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜~")
    
    if user_input:
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        user_message = {"role": "user", "content": user_input}
        st.session_state.chat_history.append(user_message)
        
        # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
        with st.chat_message("user"):
            st.markdown(f"<div class='user-message'>{user_input}</div>", unsafe_allow_html=True)
        
        # ä½¿ç”¨å ä½ç¬¦æ˜¾ç¤ºæ€è€ƒçŠ¶æ€
        thinking_placeholder = st.empty()
        thinking_placeholder.markdown(f"""
        <div class='assistant-message' style='text-align:center;'>
            ğŸ”ğŸ” æ€è€ƒä¸­...
        </div>
        """, unsafe_allow_html=True)
        
        try:
            # åˆ¤æ–­é—®é¢˜ç±»å‹
            is_sci_question = is_scientific_query(user_input)
            rag_docs = []
            
            # ç›¸å…³æ–‡æ¡£æ£€ç´¢
            if is_sci_question and st.session_state.vector_store:
                try:
                    vector_results = st.session_state.vector_store.similarity_search(user_input, k=3)
                    rag_docs.extend(vector_results)
                    
                    if st.session_state.es:
                        keyword_results = keyword_search(st.session_state.es, user_input, top_k=2)
                        rag_docs.extend(keyword_results)
                        
                    # å»é‡
                    seen_content = set()
                    unique_docs = []
                    for doc in rag_docs:
                        if doc.page_content not in seen_content:
                            seen_content.add(doc.page_content)
                            unique_docs.append(doc)
                    rag_docs = unique_docs[:3]
                except Exception as e:
                    st.error(f"æœç´¢å‡ºé”™: {str(e)}")
            
            # ç”Ÿæˆå“åº”
            response = ""
            response_container = st.empty()
            
            with st.chat_message("assistant"):
                response_placeholder = st.empty()
                full_response = ""
                
                # ç”Ÿæˆå“åº”å†…å®¹
                generator = generate_response_stream(
                    user_input, 
                    is_sci_question, 
                    rag_docs if rag_docs else None
                )
                
                # ç›´æ¥æµå¼æ›´æ–°å“åº”å ä½ç¬¦
                for chunk in generator:
                    full_response += chunk
                    # ç”¨ä¸´æ—¶å˜é‡è§£å†³åæ–œæ é—®é¢˜
                    temp_response = full_response.replace("\n", "<br>")
                    # å®æ—¶æ›´æ–°æ˜¾ç¤ºå†…å®¹ï¼ˆå¸¦æ¢è¡Œå¤„ç†ï¼‰
                    response_placeholder.markdown(
                        f"<div class='assistant-message'>{temp_response}</div>", 
                        unsafe_allow_html=True
                    )
                    # æ·»åŠ å¾®å°å»¶è¿Ÿç¡®ä¿UIæ›´æ–°
                    time.sleep(0.01)
                
                # æœ€ç»ˆç¡®ä¿å®Œæ•´å†…å®¹æ˜¾ç¤º
                # å†æ¬¡ä½¿ç”¨ä¸´æ—¶å˜é‡
                final_response = full_response.replace("\n", "<br>")
                response_placeholder.markdown(
                    f"<div class='assistant-message'>{final_response}</div>", 
                    unsafe_allow_html=True
                )
                
                # æ·»åŠ åˆ°èŠå¤©å†å²
                assistant_message = {"role": "assistant", "content": full_response}
                st.session_state.chat_history.append(assistant_message)
            
            # æ¸…é™¤æ€è€ƒçŠ¶æ€
            thinking_placeholder.empty()
        except Exception as e:
            # é”™è¯¯å¤„ç†
            error_msg = f"âš ï¸ ç³»ç»Ÿå‡ºé”™: {str(e)}"
            
            # æ˜¾ç¤ºé”™è¯¯
            with st.chat_message("assistant"):
                st.markdown(f"<div class='assistant-message'>{error_msg}</div>", unsafe_allow_html=True)
            
            # æ·»åŠ é”™è¯¯æ¶ˆæ¯åˆ°å†å²
            st.session_state.chat_history.append({"role": "assistant", "content": error_msg})
            
            # æ¸…é™¤æ€è€ƒçŠ¶æ€
            thinking_placeholder.empty()
            
    # æŒ‰é’®åˆ‡æ¢å¤„ç†ï¼ˆæ”¾åœ¨æœ€åç¡®ä¿å…¶ä»–å…ƒç´ å·²æ¸²æŸ“ï¼‰
    if button:
        st.session_state.high_contrast_mode = not st.session_state.high_contrast_mode
        st.rerun()

if __name__ == "__main__":
    # åˆ›å»ºç¼“å­˜ç›®å½•
    os.makedirs(os.path.join(os.getcwd(), "cache"), exist_ok=True)
    
    # è¿è¡Œä¸»ç¨‹åº
    main()